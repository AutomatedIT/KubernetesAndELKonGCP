#*************************************************************************************
#Installing with yum
#https://cloud.google.com/sdk/docs/downloads-yum

sudo tee -a /etc/yum.repos.d/google-cloud-sdk.repo << EOM
[google-cloud-sdk]
name=Google Cloud SDK
baseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOM

yum install google-cloud-sdk
yum install kubectl

#configure GoogleCloudPlatform
gcloud init
#*************************************************************************************
#create a k8s cluster
export CLUSTER=elk-stack-ppl-jan-2019
export ZONE=europe-west2

gcloud container clusters create "$CLUSTER" --zone "$ZONE"
gcloud container clusters create "$CLUSTER" --zone "$ZONE" --num-nodes=2 --machine-type=n1-standard-4
gcloud container clusters create "$CLUSTER" --zone "$ZONE" --num-nodes=2 --enable-autoscaling --min-nodes 1 --max-nodes 20 --machine-type=n1-standard-4

#create an admin rbac role for the new project
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=aws1_test_user@technasthai.co.uk

#*************************************************************************************
#create namespace

00-namespace.yml
~~~~
apiVersion: v1
kind: Namespace
metadata:
  name: automateditsolutions-dev
  labels:
    env: dev
~~~~
kubectl create -f 00-namespace.yml
#*************************************************************************************
#deploy and configure elasticsearch

#create rbac roles
kubectl create serviceaccount sa-elasticsearch -n automateditsolutions-dev

10-rbac.yml
~~~~
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: elasticsearch
  namespace: automateditsolutions-dev
  labels:
    env: dev
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: elasticsearch
  namespace: automateditsolutions-dev
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: elasticsearch
subjects:
- kind: ServiceAccount
  name: sa-elasticsearch
  namespace: automateditsolutions-dev
---
~~~~
kubectl create -f 10-rbac.yml

#create Services for Communication
20-services.yml

~~~~
---
apiVersion: v1
kind: Service
metadata:
  namespace: automateditsolutions-dev
  name: elasticsearch
  labels:
    env: dev
spec:
  type: ClusterIP
  selector:
    app: elasticsearch-client
  ports:
  - name: http
    port: 9200
    protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  namespace: automateditsolutions-dev
  name: elasticsearch-data
  labels:
    env: dev
spec:
  clusterIP: None
  selector:
    app: elasticsearch-data
  ports:
  - port: 9300
    name: transport
---
apiVersion: v1
kind: Service
metadata:
  namespace: automateditsolutions-dev
  name: elasticsearch-discovery
  labels:
    env: dev
spec:
  selector:
    app: elasticsearch-master
  ports:
  - name: transport
    port: 9300
    protocol: TCP
~~~~

kubectl create -f 20-services.yml

#StatefulSet for Data nodes

30-statefulset-data.yml

~~~~
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-data
  namespace: automateditsolutions-dev
  labels:
    app: elasticsearch-data
    env: dev
spec:
  serviceName: elasticsearch-data
  replicas: 1 # scale when desired
  selector:
    matchLabels:
      app: elasticsearch-data
  template:
    metadata:
      labels:
        app: elasticsearch-data
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.27.2
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch-data
        image: txn2/k8s-es:v6.2.3
        imagePullPolicy: Always
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: DISCOVERY_SERVICE
          value: elasticsearch-discovery
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: CLUSTER_NAME
          value: elasticsearch
        - name: NODE_DATA
          value: "true"
        - name: NODE_MASTER
          value: "false"
        - name: NODE_INGEST
          value: "false"
        - name: HTTP_ENABLE
          value: "false"
        - name: ES_JAVA_OPTS
          value: -Xms256m -Xmx256m
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        resources:
          limits:
            cpu: 1
        ports:
        - containerPort: 9300
          name: transport
        volumeMounts:
        - name: elasticsearch-data-storage
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data-storage
    spec:
      storageClassName: standard
      accessModes: [ ReadWriteOnce ]
      resources:
        requests:
          storage: 2Gi # small for dev / testing
~~~~
kubectl create -f 30-statefulset-data.yml

#Deployment for Master Nodes
40-deployment-master.yml

~~~~
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch-master
  namespace: automateditsolutions-dev
  labels:
    app: elasticsearch-master
    env: dev
spec:
  replicas: 2 # scale as desired (see NUMBER_OF_MASTERS below)
  selector:
    matchLabels:
      app: elasticsearch-master
  template:
    metadata:
      labels:
        app: elasticsearch-master
        env: dev
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.27.2
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch-master
        image: txn2/k8s-es:v6.2.3
        imagePullPolicy: Always
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: DISCOVERY_SERVICE
          value: elasticsearch-discovery
        - name: CLUSTER_NAME
          value: elasticsearch
        - name: NUMBER_OF_MASTERS
          value: "2"
        - name: NODE_MASTER
          value: "true"
        - name: NODE_INGEST
          value: "false"
        - name: NODE_DATA
          value: "false"
        - name: HTTP_ENABLE
          value: "false"
        - name: ES_JAVA_OPTS
          value: -Xms256m -Xmx256m
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        resources:
          limits:
            cpu: 1
        ports:
        - containerPort: 9300
          name: transport
        volumeMounts:
        - name: storage
          mountPath: /data
      volumes:
      - emptyDir:
          medium: ""
        name: "storage"
~~~~

kubectl create -f  40-deployment-master.yml


#Deployment for Client and Ingest Nodes
60-deployment-client.yml

~~~~
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch-client
  namespace: automateditsolutions-dev
  labels:
    app: elasticsearch-client
    env: dev
spec:
  replicas: 1 # scale as desired
  selector:
    matchLabels:
      app: elasticsearch-client
  template:
    metadata:
      labels:
        app: elasticsearch-client
        env: dev
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.27.2
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch-client
        image: txn2/k8s-es:v6.2.3
        imagePullPolicy: Always
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: DISCOVERY_SERVICE
          value: elasticsearch-discovery
        - name: CLUSTER_NAME
          value: elasticsearch
        - name: NODE_MASTER
          value: "false"
        - name: NETWORK_HOST
          value: "0.0.0.0"
        - name: NODE_INGEST
          value: "true"
        - name: NODE_DATA
          value: "false"
        - name: HTTP_ENABLE
          value: "true"
        - name: ES_JAVA_OPTS
          value: -Xms256m -Xmx256m
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        resources:
          limits:
            cpu: 1
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        volumeMounts:
        - name: storage
          mountPath: /data
      volumes:
      - emptyDir:
          medium: ""
        name: "storage"
~~~~

kubectl create -f 60-deployment-client.yml

#Testing
kubectl get pods --all-namespaces
kubectl exec -it elasticsearch-master-764c76864c-7sp7s sh -n automateditsolutions-dev
curl -X GET "elasticsearch:9200/_cluster/stats?human&pretty"

#*************************************************************************************
#deploy and configure kibana

#create kibana Service

20-kibana-service.yml
~~~~
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: automateditsolutions-dev
  labels:
    app: kibana
    env: dev
spec:
  selector:
    app: kibana
  ports:
  - protocol: "TCP"
    port: 80
    targetPort: 5601
  type: ClusterIP
~~~~
kubectl create -f 20-kibana-service.yml

#Kibana ConfigMap
30-configmap.yml
~~~~
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana
  namespace: automateditsolutions-dev
  labels:
    app: kibana
    env: dev
data:
  # kibana.yml is mounted into the Kibana container
  # see https://github.com/elastic/kibana/blob/master/config/kibana.yml
  # Kubernetes Ingress is used to route kib.the-project.d4ldev.txn2.com
  kibana.yml: |-
    server.name: kib.the-project.d4ldev.txn2.com
    server.host: "0"
    elasticsearch.url: http://elasticsearch:9200
~~~~
kubectl create -f 30-configmap.yml

#kibana Deployment
40-kibana-deployment.yml
~~~~
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: automateditsolutions-dev
  labels:
    app: kibana
    env: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
        env: dev
    spec:
      volumes:
      - name: kibana-config-volume
        configMap:
          name: kibana
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana-oss:6.2.4
        imagePullPolicy: Always
        volumeMounts:
        - name: kibana-config-volume
          mountPath: /usr/share/kibana/config
        env:
        - name: CLUSTER_NAME
          value: elasticsearch
        ports:
        - name: http
          containerPort: 5601
~~~~
kubectl create -f 40-kibana-deployment.yml

#Basic Auth
yum install httpd-tools
htpasswd -c ./auth automated
kubectl create secret generic automated-basic-auth --from-file auth -n automateditsolutions-dev

#TLS Certificate
#Install helm
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.1-linux-amd64.tar.gz
tar zxvf helm-v2.12.1-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/helm

#create tiller rbac role
00-tiller-rbac.yml
~~~~
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
~~~~
helm init --service-account tiller
kubectl create -f 00-tiller-rbac.yml

#Install cert-manager
helm install --name cert-manager --namespace kube-system stable/cert-manager
~~~~
==> v1/Pod(related)
NAME                           READY  STATUS             RESTARTS  AGE
cert-manager-59c884d44d-sb75m  0/1    ContainerCreating  0         0s

NOTES:
cert-manager has been deployed successfully!

In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).

More information on the different types of issuers and how to configure them
can be found in our documentation:

https://cert-manager.readthedocs.io/en/latest/reference/issuers.html

For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:

https://cert-manager.readthedocs.io/en/latest/reference/ingress-shim.html
~~~~

#Create a dev ClusterIssuer
10-cluster-issuer-letsencrypt-staging.yml
~~~~
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: john@automateditsolutions.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-staging
    # Enable the HTTP-01 challenge provider
    http01: {}
~~~~
kubectl create -f 10-cluster-issuer-letsencrypt-staging.yml

[root@localhost kibana]# kubectl create -f 10-cluster-issuer-letsencrypt-staging.yml
clusterissuer.certmanager.k8s.io/letsencrypt-staging created

#Check the status of the new ClusterIssuer
kubectl describe ClusterIssuer

#Create a production ClusterIssuer
~~~~
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: letsencrypt-production
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-v01.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: john@automateditsolutions.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-production
    # Enable the HTTP-01 challenge provider
    http01: {}
~~~~
kubectl create -f 20-cluster-issuer-letsencrypt-production.yml

#Obtain a Certificate
30-Cert-protoporos-co-uk.yml
~~~~
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: protoporos-co-uk
  namespace: automateditsolutions-dev
spec:
  secretName: protoporos-co-uk-staging-tls
  issuerRef:
    name: letsencrypt-staging
    kind: ClusterIssuer
  commonName: protoporos.co.uk
  dnsNames:
  - www.protoporos.co.uk
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - protoporos.co.uk
    - http01:
        ingress: my-ingress
      domains:
      - www.protoporos.co.uk
~~~~
kubectl create -f 30-Cert-protoporos-co-uk.yml

#Check the status
kubectl describe certificate protoporos-co-uk

#Obtain a Production Certificate
30-Cert-prod-protoporos-co-uk.yml
~~~~
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: protoporos-co-uk
  namespace: automateditsolutions-dev
spec:
  secretName: protoporos-co-uk-staging-tls
  issuerRef:
    name: letsencrypt-production
    kind: ClusterIssuer
  commonName: protoporos.co.uk
  dnsNames:
  - www.protoporos.co.uk
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - protoporos.co.uk
    - http01:
        ingress: my-ingress
      domains:
      - www.protoporos.co.uk
~~~~
kubectl create -f 30-Cert-prod-protoporos-co-uk.yml

#TLS Certificate (Optional)
50-cert.yml
~~~~
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: kib-dev-cert
  namespace: automateditsolutions-dev
spec:
  secretName: kib-dev-production-tls
  issuerRef:
    name: letsencrypt-production
    kind: ClusterIssuer
  commonName: protoporos.co.uk
  dnsNames:
  - protoporos.co.uk
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - protoporos.co.uk
    - http01:
        ingressClass: nginx
      domains:
      - protoporos.co.uk
~~~~
kubectl create -f 50-cert.yml

#Ingress
60-ingress.yml
~~~~
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  namespace: automateditsolutions-dev
  labels:
    app: kibana
    env: dev
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: kibop-basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
spec:
  rules:
  - host: protoporos.co.uk
    http:
      paths:
      - backend:
          serviceName: kibana
          servicePort: 80
        path: /
  tls:
  - hosts:
    - protoporos.co.uk
    secretName: kib-dev-production-tls
~~~~
kubectl create -f 60-ingress.yml

******************************************************
kubectl patch svc kibana \
  --namespace automateditsolutions-dev \
  --patch '{"spec": {"type": "LoadBalancer"}}'

Get the Kibana URL
SERVICE_IP=$(kubectl get svc kibana \
    --namespace automateditsolutions-dev \
    --output jsonpath='{.status.loadBalancer.ingress[0].ip}')

KIBANA_URL="http://${SERVICE_IP}:5601"
